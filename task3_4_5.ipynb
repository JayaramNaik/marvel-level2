{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47bb050f",
   "metadata": {},
   "source": [
    "# Report on Tasks Completed\n",
    "\n",
    "### TASK 1: MATLAB Machine Learning Onramp Course\n",
    "\n",
    "**Approach:**  \n",
    "I enrolled in the MATLAB Machine Learning Onramp course and completed the interactive lessons to learn the basics of machine learning using MATLAB. I focused on understanding how to handle data, train models, and evaluate their performance within the MATLAB environment.\n",
    "\n",
    "**Review:**  \n",
    "The course was well-organized and helped me get hands-on experience with machine learning concepts. The exercises were clear and helped me learn step-by-step.\n",
    "\n",
    "**Difficulties:**\n",
    "\n",
    "- Sometimes the page would reset due to internet issues, causing loss of progress.\n",
    "- Getting used to MATLAB’s syntax took some extra time since I’m not familiar with that language.\n",
    "\n",
    "**Improvements Suggested:**\n",
    "\n",
    "- It would be helpful to have downloadable materials for offline study.\n",
    " \n",
    " ### Screenshot — Course Completion\n",
    "\n",
    "![image](<Screenshot 2025-10-18 005011.png>)\n",
    "\n",
    "---\n",
    "\n",
    "### TASK 2: Kaggle Crafter – Build & Publish Your Own Dataset\n",
    "\n",
    "**Approach:**  \n",
    "I created a synthetic dataset of 100 student records using Python’s Faker library in Google Colab. The dataset includes details like student ID, name, age, gender, marks, and grades. I uploaded the dataset to Kaggle, added all necessary metadata like description, tags, license, and file details.\n",
    "\n",
    "**Review:**  \n",
    "This task helped me understand how to prepare a dataset that is clean, well-documented, and ready to be shared publicly. I learned the importance of metadata and clear documentation to make datasets usable and trustworthy.\n",
    "\n",
    "**Difficulties:**\n",
    "\n",
    "- I faced a logic problem in the dataset: the marks and grades do not match correctly. For example, one student scored 100 marks but got a grade ‘C’, while another scored 78 marks but got an ‘A’. This inconsistency reduces the credibility of the dataset.\n",
    "- Choosing the right tags on Kaggle was confusing because some of the tags I wanted were not available.\n",
    "\n",
    "**Dataset (Kaggle):** [Fake Student Information Dataset](https://www.kaggle.com/datasets/jayaramlnaik/fake-student-information-dataset)\n",
    "\n",
    "---\n",
    "\n",
    "### TASK 3: Data Detox - Data Cleaning using Pandas\n",
    "\n",
    "**Approach:**  \n",
    "I worked on cleaning the customer_data.csv dataset in the Copy_of_Datadetox.ipynb notebook. I started by loading the data and removing exact duplicate rows. Then, I fixed typos in categorical columns like Country, Gender, and PreferredDevice using replace() with dictionaries. I handled impossible values in Age and TotalPurchase by setting them to NaN if outside logical ranges (e.g., Age <0 or >100). I converted SignupDate and LastLogin to datetime, fixed temporal inconsistencies where LastLogin was before SignupDate, and imputed missing values: numerical columns with median, categorical with mode, and placeholders for unique fields like Email. Finally, I dropped rows with missing CustomerID or SignupDate, filtered out fake names (numeric entries), and saved the cleaned dataset as cleaned_customer_data.csv.\n",
    "\n",
    "**Review:**  \n",
    "This task was an excellent hands-on experience with real-world data cleaning challenges. I learned the importance of inspecting data thoroughly, using appropriate imputation strategies, and preserving data integrity. The step-by-step approach helped me understand Pandas methods for handling duplicates, inconsistencies, formatting, and missing values, preparing the data effectively for analysis or modeling.\n",
    "\n",
    "**Difficulties:**  \n",
    "Understanding the logic for temporal checks (e.g., LastLogin before SignupDate) took some thought, as did choosing between dropping vs. imputing missing values. Handling dates with errors='coerce' was new, and ensuring no nulls remained required careful verification.\n",
    "\n",
    "**Improvements Suggested:**  \n",
    "The dataset could include more diverse data types for broader practice. Adding automated validation checks or using libraries like Great Expectations for data quality could enhance the process. The tutorial format was helpful, but more emphasis on why certain strategies (e.g., median vs. mean) are chosen would be beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "### TASK 4: Anomaly Detection\n",
    "\n",
    "**Approach:**  \n",
    "I worked on detecting anomalies in the G-Flix user activity logs using the anomaly_detection.csv dataset in the Anomaly_detection.ipynb notebook. I started by loading the data and performing basic inspection with info() and describe() to identify red flags like extreme max values. I created visualizations including box plots and histograms with KDE for login_duration_min, data_accessed_MB, and files_downloaded to understand normal behavior trends. For statistical anomaly detection, I applied Z-score on the numerical features, flagging users with Z-score > 3 as suspects. For unsupervised ML, I used Isolation Forest on scaled data to detect multivariate anomalies. I compared the results to find high-confidence suspects flagged by both methods, visualized them on a scatter plot, and prepared a final report with the top 5 suspects based on data accessed and evidence.\n",
    "\n",
    "**Review:**  \n",
    "This task provided practical experience in anomaly detection for security forensics. I learned to combine statistical methods (Z-score) with ML (Isolation Forest) for robust detection, and the importance of scaling data and visualizing results. The step-by-step approach helped differentiate outliers from genuine anomalies, and building the investigative report improved storytelling skills.\n",
    "\n",
    "**Difficulties:**  \n",
    "Choosing the right contamination parameter for Isolation Forest and interpreting multivariate anomalies was tricky. Ensuring the Z-score threshold (3) was appropriate for the dataset required understanding statistical significance.\n",
    "\n",
    "**Improvements Suggested:**  \n",
    "The dataset could include more features for deeper analysis. Adding evaluation metrics like precision or using other algorithms (e.g., DBSCAN) could enhance comparison. The tutorial style was effective, but more guidance on tuning hyperparameters would be helpful.\n",
    "\n",
    "---\n",
    "\n",
    "### TASK 5: Logistic Regression from Scratch\n",
    "\n",
    "**Approach:**  \n",
    "I implemented logistic regression from scratch and compared it with Scikit-Learn using the framingham.csv dataset in the Logistic_Regression.ipynb notebook. I started by loading the data, exploring it with info(), checking missing values, target distribution, and a correlation heatmap. For preprocessing, I dropped rows with missing values, manually scaled features (standardization), added a bias column, and split into train/test sets. The scratch implementation included sigmoid, log loss, gradient descent for fitting, and prediction functions. I trained the model, visualized the loss curve, calculated metrics (accuracy, precision, recall, F1) from scratch, and created a confusion matrix heatmap. Finally, I implemented using Scikit-Learn, compared metrics, and visualized the differences.\n",
    "\n",
    "**Review:**  \n",
    "This task deepened my understanding of logistic regression mechanics, matrix operations, and gradient descent. Building from scratch showed the inner workings, while comparing with Scikit-Learn highlighted abstraction benefits. I learned to handle class imbalance, evaluate models properly, and the importance of scaling and preprocessing.\n",
    "\n",
    "**Difficulties:**  \n",
    "Implementing gradient descent and ensuring convergence without vanishing gradients was challenging. Handling the bias term in matrix form and debugging the loss function took time. Class imbalance affected recall, requiring careful metric interpretation.\n",
    "\n",
    "**Improvements Suggested:**  \n",
    "The dataset could include more balanced classes or techniques like SMOTE. Adding regularization to the scratch model would make it more robust. The step-by-step breakdown was excellent, but more on hyperparameter tuning (e.g., learning rate) would help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fc102",
   "metadata": {},
   "source": [
    "## Task 3: Data Detox - Data Cleaning using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779c28e",
   "metadata": {},
   "source": [
    "### Load and Explore Dataset\n",
    "\n",
    "Load the dataset (download and replace 'path/to/dataset.csv' with actual file path) and explore the types of issues present using Pandas methods like head(), info(), and describe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10872c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Check initial shape\n",
    "initial_shape = df.shape\n",
    "print(f\"Starting point: {initial_shape[0]} rows and {initial_shape[1]} columns.\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nData info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Additional exploration: Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "print(\"\\nUnique Countries:\", df['Country'].unique())\n",
    "print(\"Unique Genders:\", df['Gender'].unique())\n",
    "print(\"Unique Devices:\", df['PreferredDevice'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88cfa5",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "\n",
    "Handle missing values by dropping or imputing them using Pandas methods like dropna() or fillna() after inspecting their significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values before handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Numerical imputation with median\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['TotalPurchase'] = df['TotalPurchase'].fillna(df['TotalPurchase'].median())\n",
    "\n",
    "# Categorical imputation with mode\n",
    "df['Country'] = df['Country'].fillna(df['Country'].mode()[0])\n",
    "df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])\n",
    "\n",
    "# Constant imputation\n",
    "df['PreferredDevice'] = df['PreferredDevice'].fillna('unknown')\n",
    "df['Email'] = df['Email'].fillna('not_provided@example.com')\n",
    "\n",
    "# Drop rows with critical missing data\n",
    "df.dropna(subset=['CustomerID'], inplace=True)\n",
    "df.dropna(subset=['SignupDate'], inplace=True)\n",
    "df.dropna(subset=['LastLogin'], inplace=True)\n",
    "\n",
    "print(\"Missing values after handling:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eeee29",
   "metadata": {},
   "source": [
    "### Fix Inconsistencies in Text/Categorical Columns\n",
    "\n",
    "Fix inconsistencies such as case mismatches or typos in text/categorical columns using string methods like str.lower() or str.replace()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab655985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix inconsistencies in text/categorical columns\n",
    "# Fixing Country names\n",
    "country_fixes = {\n",
    "    'Indai': 'India',\n",
    "    'Canda': 'Canada'\n",
    "}\n",
    "df['Country'] = df['Country'].replace(country_fixes)\n",
    "\n",
    "# Fixing Gender typos\n",
    "gender_fixes = {\n",
    "    'Femlae': 'Female',\n",
    "    'mle': 'Male',\n",
    "    'Unknown': None\n",
    "}\n",
    "df['Gender'] = df['Gender'].replace(gender_fixes)\n",
    "\n",
    "# Fixing Device typos\n",
    "device_fixes = {\n",
    "    'dasktop': 'desktop',\n",
    "    'moblie': 'mobile'\n",
    "}\n",
    "df['PreferredDevice'] = df['PreferredDevice'].replace(device_fixes)\n",
    "\n",
    "# Verify the fixes\n",
    "print(\"Unique Countries after fix:\", df['Country'].unique())\n",
    "print(\"Unique Genders after fix:\", df['Gender'].unique())\n",
    "print(\"Unique Devices after fix:\", df['PreferredDevice'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0da97",
   "metadata": {},
   "source": [
    "### Format Columns Correctly\n",
    "\n",
    "Format columns correctly, e.g., convert dates to datetime using pd.to_datetime() and numbers to int/float using astype()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c6980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format columns correctly\n",
    "import numpy as np\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['SignupDate'] = pd.to_datetime(df['SignupDate'], errors='coerce')\n",
    "df['LastLogin'] = pd.to_datetime(df['LastLogin'], errors='coerce')\n",
    "\n",
    "# Handle impossible values\n",
    "print(\"--- Before Cleaning ---\")\n",
    "print(f\"Age Range: {df['Age'].min()} to {df['Age'].max()}\")\n",
    "print(f\"Purchase Range: {df['TotalPurchase'].min()} to {df['TotalPurchase'].max()}\")\n",
    "\n",
    "df.loc[(df['Age'] < 0) | (df['Age'] > 100), 'Age'] = np.nan\n",
    "df.loc[df['TotalPurchase'] < 0, 'TotalPurchase'] = np.nan\n",
    "\n",
    "print(\"\\n--- After Cleaning ---\")\n",
    "print(f\"New Age Range: {df['Age'].min()} to {df['Age'].max()}\")\n",
    "print(f\"New Purchase Range: {df['TotalPurchase'].min()} to {df['TotalPurchase'].max()}\")\n",
    "\n",
    "# Fix temporal logic\n",
    "time_travelers = df[df['LastLogin'] < df['SignupDate']]\n",
    "print(f\"Detected {len(time_travelers)} records where LastLogin is before SignupDate.\")\n",
    "df.loc[df['LastLogin'] < df['SignupDate'], 'LastLogin'] = pd.NaT\n",
    "\n",
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee57381",
   "metadata": {},
   "source": [
    "### Remove Duplicate Rows\n",
    "\n",
    "Remove duplicate rows using drop_duplicates() if any are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "print(\"Number of rows before removing duplicates:\", len(df))\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Number of rows after removing duplicates:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9adbd",
   "metadata": {},
   "source": [
    "### Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned dataset as a new CSV using to_csv() (replace with desired file path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out fake names\n",
    "bad_names_mask = df['Name'].astype(str).str.isnumeric()\n",
    "df = df[~bad_names_mask]\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('cleaned_customer_data.csv', index=False)\n",
    "\n",
    "print(\"Cleaned dataset saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca1fe8",
   "metadata": {},
   "source": [
    "## Task 4: Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ceb44",
   "metadata": {},
   "source": [
    "### Load and Explore Dataset\n",
    "\n",
    "Load the dataset (download and replace 'path/to/user_activity_logs.csv' with actual file path) and explore it using Pandas and visualizations like histograms or scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for Task 4\n",
    "df_activity = pd.read_csv('anomaly_detection.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"--- Forensic Evidence Overview ---\")\n",
    "print(df_activity.info())\n",
    "\n",
    "print(\"\\n--- Summary Statistics (Check the MAX values!) ---\")\n",
    "print(df_activity.describe())\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df_activity.head(10))\n",
    "\n",
    "# TODO: Add more exploration if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7f719",
   "metadata": {},
   "source": [
    "### Identify Normal Behavior Trends with Visualizations\n",
    "\n",
    "Use Matplotlib or Seaborn to create visualizations identifying normal behavior trends in user activity logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448eda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify normal behavior trends\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Box plots\n",
    "features = ['login_duration_min', 'data_accessed_MB', 'files_downloaded']\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.boxplot(y=df_activity[col], color='salmon')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histograms\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(df_activity[col], bins=30, kde=True, color='teal')\n",
    "    plt.title(f'Histogram of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c23cf",
   "metadata": {},
   "source": [
    "### Apply Statistical Anomaly Detection (Z-score/IQR)\n",
    "\n",
    "Apply statistical methods like Z-score or IQR to detect anomalies, flagging outliers based on thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply statistical anomaly detection\n",
    "from scipy import stats\n",
    "\n",
    "# Select features\n",
    "features_to_check = ['login_duration_min', 'data_accessed_MB', 'files_downloaded']\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores = np.abs(stats.zscore(df_activity[features_to_check]))\n",
    "\n",
    "# Define threshold\n",
    "threshold = 3\n",
    "outliers = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Flag suspects\n",
    "df_activity['is_statistical_anomaly'] = outliers\n",
    "\n",
    "# Extract suspects\n",
    "statistical_suspects = df_activity[df_activity['is_statistical_anomaly'] == True]\n",
    "\n",
    "print(f\"Z-Score Analysis complete. Found {len(statistical_suspects)} suspects.\")\n",
    "print(\"\\n--- Top Statistical Suspects ---\")\n",
    "print(statistical_suspects.sort_values(by='data_accessed_MB', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a964d3",
   "metadata": {},
   "source": [
    "### Apply Unsupervised ML Anomaly Detection (Isolation Forest/DBSCAN)\n",
    "\n",
    "Apply unsupervised ML techniques like Isolation Forest or DBSCAN from Scikit-Learn, scaling data first if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d258d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unsupervised ML anomaly detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature scaling\n",
    "features = ['login_duration_min', 'data_accessed_MB', 'files_downloaded']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_activity[features])\n",
    "\n",
    "# Isolation Forest\n",
    "model = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\n",
    "df_activity['ml_anomaly_score'] = model.fit_predict(X_scaled)\n",
    "df_activity['is_ml_anomaly'] = df_activity['ml_anomaly_score'].map({1: False, -1: True})\n",
    "\n",
    "# Extract suspects\n",
    "ml_suspects = df_activity[df_activity['is_ml_anomaly'] == True]\n",
    "\n",
    "print(f\"ML Investigation complete. Isolation Forest flagged {len(ml_suspects)} suspects.\")\n",
    "print(\"\\n--- Top ML Suspects ---\")\n",
    "print(ml_suspects.sort_values(by='data_accessed_MB', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150431e2",
   "metadata": {},
   "source": [
    "### Compare Flagged Anomalies\n",
    "\n",
    "Compare anomalies flagged by both methods using visualizations and lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare flagged anomalies\n",
    "# High-confidence suspects\n",
    "df_activity['is_high_confidence'] = df_activity['is_statistical_anomaly'] & df_activity['is_ml_anomaly']\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_activity[df_activity['is_ml_anomaly'] == False],\n",
    "                x='data_accessed_MB', y='files_downloaded',\n",
    "                alpha=0.4, label='Normal Behavior', color='gray')\n",
    "sns.scatterplot(data=df_activity[df_activity['is_ml_anomaly'] == True],\n",
    "                x='data_accessed_MB', y='files_downloaded',\n",
    "                color='orange', label='ML Flags', s=80)\n",
    "top_5 = df_activity.sort_values(by=['is_high_confidence', 'data_accessed_MB'], ascending=False).head(5)\n",
    "sns.scatterplot(data=top_5, x='data_accessed_MB', y='files_downloaded',\n",
    "                color='red', marker='X', s=200, label='Top 5 Suspects')\n",
    "plt.title('G-Flix Forensic Map: Spotting the Breach')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d845fb5",
   "metadata": {},
   "source": [
    "### Prepare Final Report with Top 5 Suspects\n",
    "\n",
    "Prepare a report listing top 5 suspects with evidence, justifying based on multiple features and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final report\n",
    "print(\"--- G-FLIX BOARD OF DIRECTORS: TOP 5 SUSPECTS REPORT ---\")\n",
    "report_cols = ['user_id', 'login_duration_min', 'data_accessed_MB', 'files_downloaded', 'remote_access']\n",
    "print(top_5[report_cols])\n",
    "\n",
    "# Additional details\n",
    "for i, suspect in enumerate(top_5.itertuples(), 1):\n",
    "    print(f\"\\nSuspect {i}: {suspect.user_id}\")\n",
    "    print(f\"Evidence: Login {suspect.login_duration_min} min, Data {suspect.data_accessed_MB} MB, Files {suspect.files_downloaded}, Remote {suspect.remote_access}\")\n",
    "    print(\"Justification: Flagged by both statistical and ML methods as high-confidence anomaly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af321309",
   "metadata": {},
   "source": [
    "## Task 5: Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e7f32",
   "metadata": {},
   "source": [
    "### Implement Logistic Regression from Scratch\n",
    "\n",
    "Implement logistic regression from scratch using NumPy for matrix operations, gradient descent, and sigmoid function on the heart disease dataset (replace with actual file path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Logistic Regression from Scratch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df_heart = pd.read_csv('framingham.csv')\n",
    "\n",
    "# Preprocessing\n",
    "df_clean = df_heart.dropna()\n",
    "X = df_clean.drop('TenYearCHD', axis=1).values\n",
    "y = df_clean['TenYearCHD'].values\n",
    "\n",
    "# Feature scaling\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias column\n",
    "intercept = np.ones((X_scaled.shape[0], 1))\n",
    "X_final = np.concatenate((intercept, X_scaled), axis=1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Compute loss\n",
    "def compute_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Fit logistic regression\n",
    "def fit_logistic_regression(X, y, lr=0.1, iterations=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    loss_history = []\n",
    "    for i in range(iterations):\n",
    "        z = np.dot(X, weights)\n",
    "        y_pred = sigmoid(z)\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "        weights -= lr * dw\n",
    "        current_loss = compute_loss(y, y_pred)\n",
    "        loss_history.append(current_loss)\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {current_loss:.4f}\")\n",
    "    return weights, loss_history\n",
    "\n",
    "# Predict\n",
    "def predict(X, weights, threshold=0.5):\n",
    "    probabilities = sigmoid(np.dot(X, weights))\n",
    "    return (probabilities >= threshold).astype(int)\n",
    "\n",
    "# Train the model\n",
    "final_weights, history = fit_logistic_regression(X_train, y_train, lr=0.1, iterations=2000)\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b545a",
   "metadata": {},
   "source": [
    "### Implement Logistic Regression Using Scikit-Learn\n",
    "\n",
    "Implement logistic regression using Scikit-Learn's LogisticRegression on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement using Scikit-Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train model\n",
    "sk_model = LogisticRegression(penalty=None, max_iter=2000)\n",
    "sk_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_sk = sk_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d129c",
   "metadata": {},
   "source": [
    "### Compare Models with Metrics\n",
    "\n",
    "Compare models using metrics like accuracy, precision, recall, and F1-score from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# For scratch model\n",
    "y_pred_scratch = predict(X_test, final_weights)\n",
    "acc_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "prec_scratch = precision_score(y_test, y_pred_scratch)\n",
    "rec_scratch = recall_score(y_test, y_pred_scratch)\n",
    "f1_scratch = f1_score(y_test, y_pred_scratch)\n",
    "\n",
    "# For sklearn\n",
    "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
    "prec_sk = precision_score(y_test, y_pred_sk)\n",
    "rec_sk = recall_score(y_test, y_pred_sk)\n",
    "f1_sk = f1_score(y_test, y_pred_sk)\n",
    "\n",
    "# Comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Scratch': [acc_scratch, prec_scratch, rec_scratch, f1_scratch],\n",
    "    'Scikit-Learn': [acc_sk, prec_sk, rec_sk, f1_sk]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"--- MODEL COMPARISON ---\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visual comparison\n",
    "comparison_df.set_index('Metric').plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Performance Comparison: Scratch vs. Scikit-Learn\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97af486",
   "metadata": {},
   "source": [
    "### Discuss Performance Differences\n",
    "\n",
    "Discuss performance differences, training time, implementation difficulty, and interpretability between scratch and library implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9122a5c",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "- **Performance Differences:** The Scikit-Learn model may have slightly better or similar metrics due to optimized algorithms, but the scratch model performs comparably, showing the implementation is correct.\n",
    "- **Training Time:** Scratch model took longer to train due to manual iterations, while Scikit-Learn is faster with built-in optimizations.\n",
    "- **Implementation Difficulty:** Scratch required understanding matrix math, sigmoid, and gradient descent, making it harder but educational. Scikit-Learn abstracts this complexity.\n",
    "- **Interpretability:** Both are interpretable via weights, but scratch gives more control over the process."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
